{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tolerant_retrieval:\n",
    "    \n",
    "    \n",
    "    def start(self, cells):\n",
    "        hindi_document = []\n",
    "        for row in cells.iter_rows():\n",
    "            index = 0;\n",
    "            for k in row:\n",
    "                if index == 1:\n",
    "                    hindi_document.append(k.internal_value.lower())\n",
    "                    break\n",
    "                index = index + 1\n",
    "        tokens = self.generate_tokens(hindi_document)\n",
    "        pruned_tokens = self.remove_special_words(tokens)\n",
    "        little_tokens = self.remove_stop_words(pruned_tokens)\n",
    "        #stemmed_tokens = stemming(little_tokens)\n",
    "        \n",
    "        return little_tokens\n",
    "\n",
    "    def generate_tokens(self, english_document):\n",
    "        final_tokens = []\n",
    "        for i in range(len(english_document)):\n",
    "\n",
    "            nltk_tokens = nltk.word_tokenize(english_document[i])\n",
    "            #print(nltk_tokens)\n",
    "            final_tokens.append(nltk_tokens)\n",
    "        #rint(final_tokens)\n",
    "        return final_tokens\n",
    "\n",
    "    def remove_special_words(self, tokens):\n",
    "        final_tokens = []\n",
    "        special_characters = [',', ':', '-', '.', '(', ')', '[', ']', '{', '}', '|', '!', '@', '#', '$', '``','%', '^', '&', '*', '`', '~', ';', '<', '>', '|', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ii', 'iii']\n",
    "\n",
    "        for i in range(len(tokens)):\n",
    "            temp = []\n",
    "            for j in range(len(tokens[i])):\n",
    "                word = tokens[i][j].strip(' ')\n",
    "\n",
    "                if word not in special_characters:\n",
    "                    if word.isdigit():\n",
    "                        t = word\n",
    "                    elif word != \"\": \n",
    "                        temp.append(word)\n",
    "\n",
    "            final_tokens.append(temp)\n",
    "\n",
    "        return final_tokens    \n",
    "    \n",
    "    def remove_stop_words(self, pruned_token):\n",
    "        f=codecs.open(\"stopwords.txt\",encoding='utf-8')\n",
    "        little_pruned_tokens = []\n",
    "\n",
    "        stopwords = [x.strip() for x in f.readlines()]\n",
    "\n",
    "        for i in range(len(pruned_token)):\n",
    "            temp = []\n",
    "            for j in range(len(pruned_token[i])):\n",
    "                if pruned_token[i][j] not in stopwords:\n",
    "                    temp.append(pruned_token[i][j])    \n",
    "\n",
    "            little_pruned_tokens.append(temp)\n",
    "\n",
    "\n",
    "        return little_pruned_tokens    \n",
    "\n",
    "    def generate_stem_words(self, word):\n",
    "        suffixes = {\n",
    "            1: [u\"ो\",u\"े\",u\"ू\",u\"ु\",u\"ी\",u\"ि\",u\"ा\"],\n",
    "            2: [u\"कर\",u\"ाओ\",u\"िए\",u\"ाई\",u\"ाए\",u\"ने\",u\"नी\",u\"ना\",u\"ते\",u\"ीं\",u\"ती\",u\"ता\",u\"ाँ\",u\"ां\",u\"ों\",u\"ें\"],\n",
    "            3: [u\"ाकर\",u\"ाइए\",u\"ाईं\",u\"ाया\",u\"ेगी\",u\"ेगा\",u\"ोगी\",u\"ोगे\",u\"ाने\",u\"ाना\",u\"ाते\",u\"ाती\",u\"ाता\",u\"तीं\",u\"ाओं\",u\"ाएं\",u\"ुओं\",u\"ुएं\",u\"ुआं\"],\n",
    "            4: [u\"ाएगी\",u\"ाएगा\",u\"ाओगी\",u\"ाओगे\",u\"एंगी\",u\"ेंगी\",u\"एंगे\",u\"ेंगे\",u\"ूंगी\",u\"ूंगा\",u\"ातीं\",u\"नाओं\",u\"नाएं\",u\"ताओं\",u\"ताएं\",u\"ियाँ\",u\"ियों\",u\"ियां\"],\n",
    "            5: [u\"ाएंगी\",u\"ाएंगे\",u\"ाऊंगी\",u\"ाऊंगा\",u\"ाइयाँ\",u\"ाइयों\",u\"ाइयां\"],\n",
    "        }\n",
    "\n",
    "        for L in 5, 4, 3, 2, 1:\n",
    "            if len(word) > L + 1:\n",
    "                 for suf in suffixes[L]:\n",
    "                    if word.endswith(suf):\n",
    "                        return word[:-L]\n",
    "        return word\n",
    "\n",
    "    def stemming(self, little_tokens):\n",
    "\n",
    "        stemmed_words = []\n",
    "\n",
    "        for i in range(len(little_tokens)):\n",
    "            temp = []\n",
    "            for j in range(len(little_tokens[i])):        \n",
    "                temp.append(self.generate_stem_words(little_tokens[i][j]))\n",
    "\n",
    "            stemmed_words.append(temp)\n",
    "\n",
    "        return stemmed_words\n",
    "\n",
    "    def dictionary_of_words(self, stemmed_words):\n",
    "    \n",
    "        dictionary = {}\n",
    "\n",
    "        for i in range(len(stemmed_words)):\n",
    "            for j in range(len(stemmed_words[i])):        \n",
    "                #count = frequency.get(stemmed_words[i][j],0)\n",
    "                dictionary[stemmed_words[i][j]] = 1\n",
    "        \n",
    "        sorted_dict = self.sortedDictkeys(dictionary)\n",
    "        \n",
    "        return sorted_dict\n",
    "    \n",
    "    def sortedDictkeys(self, adict):\n",
    "        sort_dict = {}\n",
    "\n",
    "        for (key, value) in sorted(adict.items()):\n",
    "            sort_dict[key] = value\n",
    "\n",
    "        return sort_dict\n",
    "    \n",
    "    def bigrams_postings(self, dictionary):\n",
    "\n",
    "        bigram = defaultdict(list)\n",
    "\n",
    "        for key, date in dictionary.items():\n",
    "            for i in range(len(key)-1):\n",
    "                bivalue = key[i:i+2]\n",
    "                bigram[bivalue].append(key)\n",
    "\n",
    "        return bigram\n",
    "    \n",
    "    def jaccard_similarity(self, dict_word, query_word):\n",
    "        list1 = []\n",
    "        list2 = []\n",
    "\n",
    "        for i in range(len(dict_word)-1):\n",
    "            list1.append(dict_word[i:i+2]) \n",
    "\n",
    "        for i in range(len(query_word)-1):\n",
    "            list2.append(query_word[i:i+2]) \n",
    "\n",
    "        intersection = len(set(list1).intersection(set(list2)))\n",
    "        union = (len(dict_word)-1) + (len(query_word)) - intersection\n",
    "\n",
    "        return float(intersection/union)\n",
    "    \n",
    "    def suggested_corrected_words(self, bigram_dict, query_word):\n",
    "    \n",
    "        selected_word = set()\n",
    "        for i in range(len(query_word)-1):\n",
    "            key = query_word[i:i+2]\n",
    "\n",
    "            for word in bigram_dict[key]:\n",
    "                similarity = self.jaccard_similarity(word, query_word)\n",
    "                #x = float(nltk.jaccard_distance(set(word), set(query_word)))\n",
    "\n",
    "                if similarity > 0.30:\n",
    "                #if 1-x > 0.70:\n",
    "                    selected_word.add(word)\n",
    "                    #print(word)\n",
    "            \n",
    "            min_len = 80\n",
    "            correct_word = \"\"\n",
    "            for word in selected_word:\n",
    "                #print(word)\n",
    "                dist = self.edit_distance(word, query_word)\n",
    "                #print(word, dist)\n",
    "                if min_len >= dist:\n",
    "                    min_len = dist\n",
    "                    correct_word = word\n",
    "\n",
    "        return correct_word\n",
    "    \n",
    "    def edit_distance(self, word, query_word):\n",
    "    \n",
    "        if word == query_word: return 0\n",
    "        elif len(word) == 0: return len(query_word)\n",
    "        elif len(query_word) == 0: return len(word)\n",
    "        dp0 = [None] * (len(query_word) + 1)\n",
    "        dp1 = [None] * (len(query_word) + 1)\n",
    "        for i in range(len(dp0)):\n",
    "            dp0[i] = i\n",
    "        for i in range(len(word)):\n",
    "            dp1[0] = i + 1\n",
    "            for j in range(len(query_word)):\n",
    "                cost = 0 if word[i] == query_word[j] else 1\n",
    "                dp1[j + 1] = min(dp1[j] + 1, dp0[j + 1] + 1, dp0[j] + cost)\n",
    "            for j in range(len(dp0)):\n",
    "                dp0[j] = dp1[j]\n",
    "                \n",
    "        return dp1[len(query_word)]\n",
    "    \n",
    "    def boolean_retrieval(self, bigram_dict, queries, inverted_index):\n",
    "        \n",
    "        ps = PorterStemmer()\n",
    "        bool_result = []\n",
    "        correct_queries = []\n",
    "        \n",
    "        for i in range(len(queries)):\n",
    "            connecting_words = []\n",
    "            cnt = 1\n",
    "            different_words = []\n",
    "\n",
    "            for word in queries[i]:\n",
    "                if word.lower() != \"तथा\" and word.lower() != \"अथवा\" and word.lower() != \"नहीं\":\n",
    "                    different_words.append(word.lower())\n",
    "                else:\n",
    "                    connecting_words.append(word.lower())\n",
    "#             print(connecting_words)\n",
    "#             print(different_words)\n",
    "#             break\n",
    "            \n",
    "            for j in range(len(different_words)):\n",
    "                different_words[j] = self.suggested_corrected_words(bigram_dict, different_words[j])\n",
    "                #print(different_words[j])\n",
    "                \n",
    "            result = inverted_index[ps.stem(different_words[0])]\n",
    "            result_query = different_words[0]\n",
    "            \n",
    "            if len(different_words) == 1 and len(connecting_words) == 1:\n",
    "                result = self.not_merging_list(result)\n",
    "                result_query = 'नहीं' + ' ' + result_query\n",
    "                \n",
    "            else:\n",
    "                m = 1\n",
    "                k = 0\n",
    "                while k != len(connecting_words):\n",
    "                    flag = 1\n",
    "                    #print(k, m)\n",
    "                    list1 = inverted_index[ps.stem(different_words[m])]\n",
    "                    #print(list1)\n",
    "                    if connecting_words[k] == 'तथा':\n",
    "                        if k+1 < len(connecting_words) and connecting_words[k+1] == 'नहीं':\n",
    "                            list2 = self.not_merging_list(list1)\n",
    "                            result = self.and_merging_list(result, list2)\n",
    "                            flag = 0\n",
    "                            op = 'तथा'\n",
    "                            k = k+1\n",
    "                        else:\n",
    "                            result = self.and_merging_list(result, list1)\n",
    "                    elif connecting_words[k] == 'अथवा':\n",
    "                        if k+1 < len(connecting_words) and connecting_words[k+1] == 'नहीं':\n",
    "                            list2 = self.not_merging_list(list1)\n",
    "                            result = self.or_merging_list(result, list2)\n",
    "                            flag = 0\n",
    "                            op = 'अथवा'\n",
    "                            k = k+1\n",
    "                        else:\n",
    "                            result = self.or_merging_list(result, list1)\n",
    "                    else:\n",
    "                        result = not_merging_list(result, list1)\n",
    "\n",
    "                    if flag == 1:\n",
    "                        result_query = result_query + \" \" + connecting_words[k] + \" \" + different_words[m]\n",
    "                    else:\n",
    "                        result_query = result_query + \" \" + op + \" \" + connecting_words[k] + \" \" + different_words[m]\n",
    "                    m = m+1 \n",
    "                    k = k+1\n",
    "\n",
    "            bool_result.append(result)\n",
    "            correct_queries.append(result_query)\n",
    "\n",
    "        return bool_result, correct_queries\n",
    "\n",
    "    def not_merging_list(self, list2):\n",
    "        result = []\n",
    "        k = 0\n",
    "        for i in range(2276):\n",
    "            #print(i, list2[k])\n",
    "            if k < len(list2) and i == list2[k]:\n",
    "                k = k+1\n",
    "            else:\n",
    "                result.append(i)\n",
    "                \n",
    "        return result\n",
    "    \n",
    "    def and_merging_list(self, list1, list2):\n",
    "        i = 0\n",
    "        j = 0\n",
    "        result = []\n",
    "\n",
    "        while i != len(list1) and j != len(list2):\n",
    "            if list1[i] == list2[j]:\n",
    "                result.append(list1[i])\n",
    "                i = i+1\n",
    "                j = j+1\n",
    "            elif list1[i] > list2[j]:\n",
    "                j = j+1\n",
    "            else:\n",
    "                i = i+1\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def or_merging_list(self, list1, list2):\n",
    "        i = 0\n",
    "        j = 0\n",
    "        result = []\n",
    "\n",
    "        while i != len(list1) and j != len(list2):\n",
    "            if list1[i] == list2[j]:\n",
    "                result.append(list1[i])\n",
    "                i = i+1\n",
    "                j = j+1\n",
    "            elif list1[i] > list2[j]:\n",
    "                result.append(list2[j])\n",
    "                j = j+1\n",
    "            else:\n",
    "                result.append(list1[i])\n",
    "                i = i+1\n",
    "\n",
    "        while i != len(list1):\n",
    "            result.append(list1[i])\n",
    "            i = i+1\n",
    "\n",
    "        while j != len(list2):\n",
    "            result.append(list2[j])\n",
    "            j = j+1\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def inverted_index_fun(self, documents):\n",
    "\n",
    "        inverted_index = defaultdict(set)\n",
    "\n",
    "        for i in range(len(documents)):\n",
    "            for j in range(len(documents[i])):\n",
    "\n",
    "                inverted_index[documents[i][j]].add(i)\n",
    "\n",
    "        #sorting the dictionary by values:\n",
    "        for keys, value in inverted_index.items():\n",
    "            tmp = sorted(value)\n",
    "            inverted_index[keys] = tmp\n",
    "\n",
    "        return inverted_index\n",
    "    \n",
    "    def get_documents_postings(self, word):\n",
    "        return self.inverted_index_fun(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dell/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/dell/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import openpyxl as px\n",
    "import codecs\n",
    "import string\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import defaultdict\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    W = px.load_workbook('Dand_Prakriya.xlsx')\n",
    "    p = W.get_sheet_by_name(name = 'Sheet')\n",
    "    hindi_document = []\n",
    "    \n",
    "    ret = Tolerant_retrieval()\n",
    "    \n",
    "    pruned_tokens = ret.start(p)\n",
    "    stemmed_words = ret.stemming(pruned_tokens)\n",
    "    inverted_index = ret.inverted_index_fun(stemmed_words)\n",
    "    #print(inverted_index[\"government\"])\n",
    "    #print(inverted_index[\"state\"])\n",
    "    dictionary = ret.dictionary_of_words(pruned_tokens)\n",
    "    #print(dictionary)\n",
    "    #correcting queries:\n",
    "    bigram_dict = ret.bigrams_postings(dictionary)\n",
    "    \n",
    "    f=codecs.open(\"query_hin.txt\",encoding='utf-8')\n",
    "    search = [x.strip() for x in f.readlines()]\n",
    "    queries = ret.generate_tokens(search)\n",
    "    \n",
    "    boolean_docs, correct_queries = ret.boolean_retrieval(bigram_dict, queries, inverted_index)\n",
    "    \n",
    "    return boolean_docs, correct_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dell/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated function get_sheet_by_name (Use wb[sheetname]).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    boolean_docs, correct_queries = main()\n",
    "\n",
    "    std_out = sys.stdout\n",
    "    file = open('OUT_HINDI.txt', 'w')\n",
    "    sys.stdout = file\n",
    "\n",
    "    for i in range(len(correct_queries)):\n",
    "        print(correct_queries[i], \"=>\", boolean_docs[i])\n",
    "\n",
    "    sys.stdout = std_out\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
